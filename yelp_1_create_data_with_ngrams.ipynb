{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (20.0.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.13.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.4.5)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.13.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from multiprocessing import  Pool\n",
    "\n",
    "# Import from nltk\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import bigrams, FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load in the data and create the required structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "load_path = '../Datasets/yelp/'\n",
    "labelled_data = pd.read_csv(load_path + 'labeled_data.csv')\n",
    "test_data = pd.read_csv(load_path + 'test_data.csv')\n",
    "unlabelled_data = pd.read_csv(load_path + 'unlabeled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Had a good experience when my wife and I sat a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On my first to Montreal with my gf we came her...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One of our favorite places to go when it's col...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The doctor was very nice, got in in a good amo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Nook is an immediate phoenix staple!  I ca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Had a good experience when my wife and I sat a...      0\n",
       "1  On my first to Montreal with my gf we came her...      0\n",
       "2  One of our favorite places to go when it's col...      0\n",
       "3  The doctor was very nice, got in in a good amo...      0\n",
       "4  The Nook is an immediate phoenix staple!  I ca...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add label of 0 to all unlabeled data\n",
    "unlabelled_data['label'] = 0\n",
    "test_data['label'] = 6\n",
    "\n",
    "# Have a look at the first 5 rows\n",
    "unlabelled_data.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataframe:  700000\n"
     ]
    }
   ],
   "source": [
    "# Join the dataframes\n",
    "all_data = labelled_data.append(test_data)\n",
    "all_data = all_data.append(unlabelled_data)\n",
    "\n",
    "# Look at the length of all_data\n",
    "print('Length of dataframe: ', len(all_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Have a look at the stop words\n",
    "stop_words = stopwords.words('english') # NLTK stop_words\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove certain words from the list of stop words\n",
    "words_to_remove = [\"not\", \"too\", \"very\", \"don\", \"don't\", \"should\", \"should've\", \"aren\", \"aren't\", \n",
    "                   \"couldn\", \"couldn't\", \"didn\", \"didn't\", \"doesn\", \"doesn't\", \"hadn\", \"hadn't\", \n",
    "                   \"hasn\", \"hasn't\", \"haven\", \"haven't\", \"isn\", \"isn't\", \"shouldn\", \"shouldn't\", \n",
    "                   \"wasn\", \"wasn't\", \"weren\", \"weren't\", \"won\", \"won't\", \"wouldn\", \"wouldn't\", \n",
    "                   \"shan\", \"shan't\", \"mightn\", \"mightn't\", \"mustn\", \"mustn't\", \"needn\", \"needn't\"]\n",
    "for word in words_to_remove:\n",
    "    stop_words.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'only', 'own', 'same', 'so', 'than', 's', 't', 'can', 'will', 'just', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'ma']\n"
     ]
    }
   ],
   "source": [
    "# Have a look at the stop words\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create the tokenizer\n",
    "def tokenize(review):\n",
    "    review = review.lower() # make everything lower case\n",
    "    tokens = word_tokenize(review) # use NLTK tokenizer to generate tokens\n",
    "    tokens = [token for token in tokens if bool(re.search(r'\\w{1,}', token))] # remove tokens that don't have letters or numbers in them\n",
    "    tokens = [token for token in tokens if not token in stop_words] # remove stop words\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens] # return each token in their base form\n",
    "    \n",
    "    return ' '.join(tokens) # return the review tokenized review as a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parallelization function\n",
    "def parallelize_dataframe(df, func, n_cores=12):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to convert strings to arrays\n",
    "def prallel_tokenize(df):\n",
    "    df['text'] = df['text'].apply(tokenize)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the reviews\n",
    "all_data = parallelize_dataframe(all_data, prallel_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>new rule waiting table almost always cant wait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>flirted giving two star 's pretty damning rati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>staying planet hollywood across street saw goo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      4  new rule waiting table almost always cant wait...\n",
       "1      3  flirted giving two star 's pretty damning rati...\n",
       "2      5  staying planet hollywood across street saw goo..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at the top 3 rows\n",
    "all_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Add ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe\n",
    "data_with_ngrams = all_data.copy()\n",
    "\n",
    "# Replace non-string reviews with strings\n",
    "data_with_ngrams['text'] = data_with_ngrams['text'].apply(lambda review: str(review))\n",
    "\n",
    "# Create a dataframe for all the bigrams\n",
    "bigrams_used = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add bi-grams\n",
    "def add_bi_grams(review):\n",
    "    review = f' {review} '\n",
    "    global bi_grams_to_use\n",
    "    global bigrams_used\n",
    "    for bi_gram in bi_grams_to_use:\n",
    "        checkfor_bi_gram = f' {\" \".join(bi_gram)} '\n",
    "        if checkfor_bi_gram in review:\n",
    "            review = review.replace(checkfor_bi_gram, f' {\"\".join(bi_gram)} ')\n",
    "            bigrams_used.append(bi_gram)\n",
    "    return(review[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Unique tokens: 357816\n",
      "1. Top 200 bigrams found\n",
      "1. Bigrams added\n",
      "2. Unique tokens: 357963\n",
      "2. Top 200 bigrams found\n",
      "2. Bigrams added\n",
      "3. Unique tokens: 358117\n",
      "3. Top 200 bigrams found\n",
      "3. Bigrams added\n",
      "4. Unique tokens: 358277\n",
      "4. Top 200 bigrams found\n",
      "4. Bigrams added\n",
      "5. Unique tokens: 358441\n",
      "5. Top 200 bigrams found\n",
      "5. Bigrams added\n",
      "6. Unique tokens: 358607\n",
      "6. Top 200 bigrams found\n",
      "6. Bigrams added\n",
      "7. Unique tokens: 358790\n",
      "7. Top 200 bigrams found\n",
      "7. Bigrams added\n",
      "8. Unique tokens: 358962\n",
      "8. Top 200 bigrams found\n",
      "8. Bigrams added\n",
      "9. Unique tokens: 359141\n",
      "9. Top 200 bigrams found\n",
      "9. Bigrams added\n",
      "10. Unique tokens: 359319\n",
      "10. Top 200 bigrams found\n",
      "10. Bigrams added\n",
      "11. Unique tokens: 359502\n",
      "11. Top 200 bigrams found\n",
      "11. Bigrams added\n",
      "12. Unique tokens: 359681\n",
      "12. Top 200 bigrams found\n",
      "12. Bigrams added\n",
      "13. Unique tokens: 359865\n",
      "13. Top 200 bigrams found\n",
      "13. Bigrams added\n",
      "14. Unique tokens: 360050\n",
      "14. Top 200 bigrams found\n",
      "14. Bigrams added\n",
      "15. Unique tokens: 360234\n",
      "15. Top 200 bigrams found\n",
      "15. Bigrams added\n",
      "Final. Unique tokens: 360419\n"
     ]
    }
   ],
   "source": [
    "n = 200 # number of bigrams to be added in each itteration\n",
    "times = 15 # number of itterations\n",
    "for i in range(times):\n",
    "    # Turn reviews into one long list of tokens\n",
    "    reviews_one_long_list = (' '.join(list(data_with_ngrams['text']))).split(' ')\n",
    "\n",
    "    # Print number of unique tokens\n",
    "    print(f'{i + 1}. Unique tokens: {len(set(reviews_one_long_list))}')\n",
    "\n",
    "    # Create a freq-dist of bi-grams\n",
    "    fdist_bi_grams = FreqDist(bigrams(reviews_one_long_list))\n",
    "\n",
    "    # Get the top n bi-grams\n",
    "    top_n_bigrams = fdist_bi_grams.most_common(n)\n",
    "    bi_grams_to_use = [bigram[0] for bigram in top_n_bigrams]\n",
    "\n",
    "    # Print this step is done\n",
    "    print(f'{i + 1}. Top {n} bigrams found')\n",
    "\n",
    "    # Add the bi-grams to the text\n",
    "    data_with_ngrams['text'] = data_with_ngrams['text'].apply(add_bi_grams)\n",
    "\n",
    "    # Print finished\n",
    "    print(f'{i + 1}. Bigrams added')\n",
    "    \n",
    "# Turn reviews into one long list\n",
    "reviews_one_long_list = (' '.join(list(data_with_ngrams['text']))).split(' ')\n",
    "\n",
    "# Print number of unique tokens\n",
    "print(f'Final. Unique tokens: {len(set(reviews_one_long_list))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the bigrams to csv\n",
    "not_added_yet = set(bigrams_used)\n",
    "unique_bigrams_used = []\n",
    "for bi_gram in bigrams_used:\n",
    "    try:\n",
    "        if (bi_gram in not_added_yet):\n",
    "            unique_bigrams_used.append(list(bi_gram))\n",
    "            not_added_yet.remove(bi_gram)\n",
    "    except:\n",
    "        pass\n",
    "unique_bigrams_used = pd.DataFrame(unique_bigrams_used, columns=[0,1])\n",
    "unique_bigrams_used.to_csv(f'./bigrams.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Remove unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360419"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all the tokens and their counts\n",
    "all_tokens = [review for review in data_with_ngrams['text']]\n",
    "all_tokens = (' '.join(all_tokens)).split(' ')\n",
    "tokens_count = FreqDist(all_tokens)\n",
    "\n",
    "# Get number of all tokens\n",
    "len(set(all_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103401\n"
     ]
    }
   ],
   "source": [
    "# Get the tokens that appear more than the cutoff amount\n",
    "# Getting common tokens instead of rare tokens speeds up processing time\n",
    "cut_off = 2\n",
    "common_tokens = [token[0] for token in tokens_count.items() if token[1] > cut_off]\n",
    "\n",
    "# Get number of common tokens\n",
    "print(len(common_tokens))\n",
    "\n",
    "# Turn it into a dict for faster processing\n",
    "common_tokens_dict = {}\n",
    "for token in common_tokens:\n",
    "    common_tokens_dict[token] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to remove unique tokens\n",
    "def remove_rare_tokens(string):\n",
    "    tokens_splitted = string.split(' ')\n",
    "    tokens_unique_removed = []\n",
    "    for token in tokens_splitted:\n",
    "        try:\n",
    "            tokens_unique_removed.append(common_tokens_dict[token])\n",
    "        except:\n",
    "            pass\n",
    "    return(' '.join(tokens_unique_removed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe\n",
    "data_processed = data_with_ngrams.copy()\n",
    "\n",
    "# Remove rare tokens from the reviews\n",
    "data_processed['text'] = data_processed['text'].apply(remove_rare_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data with ngrams\n",
    "save_path = '../Datasets/yelp/'\n",
    "data_processed.to_csv(save_path + 'data_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
