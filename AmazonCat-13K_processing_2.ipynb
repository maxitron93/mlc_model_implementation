{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create embedding matrix\n",
    "We'll use the word2vec module from gensim to create an embedding matrix that can be used by tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from gensim.models.word2vec import FAST_VERSION\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "data_list = [a, b, c, d, e, f, g, h, i ,f] = [None, None, None, None, None, None, None, None, None, None]\n",
    "# data_list = [a] = [None]\n",
    "data_location = '../Datasets/AmazonCat-13K/processed/' \n",
    "for i in range(len(data_list)):\n",
    "    data_list[i] = pd.read_csv(data_location + f'first_pass_no{i + 1}.csv', encoding='latin1')\n",
    "    \n",
    "# Concatenate all the data and reset the index\n",
    "data = pd.concat(data_list, sort=False)\n",
    "data = data.reset_index()\n",
    "\n",
    "# Delete unused var (to save memory)\n",
    "del data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels from string to array (return unique values only)\n",
    "data['labels'] = data['labels'].apply(lambda labels: list(set(ast.literal_eval(labels))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "data = data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to join title and description\n",
    "def join_title_and_description(row):\n",
    "    return f'{row[\"title\"]} {row[\"description\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column that combines the title and description\n",
    "data['title_and_description'] = data.apply(lambda row: join_title_and_description(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop title and description columns (to save memory)\n",
    "data = data.drop(labels=['title', 'description'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1494407, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at the shape\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>item_id</th>\n",
       "      <th>labels</th>\n",
       "      <th>title_and_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ID:B0027DQHA0</td>\n",
       "      <td>[Music, Classical, Movies &amp; TV, TV]</td>\n",
       "      <td>Sao Paulo Samba (2008) Conducted by John Nesch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ID:0756400120</td>\n",
       "      <td>[General, Anthologies, United States, Literatu...</td>\n",
       "      <td>Past Imperfect (Daw Book Collectors) This fast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>ID:B00024YAOQ</td>\n",
       "      <td>[Business &amp; Investing, Motivation &amp; Self-Impro...</td>\n",
       "      <td>Winning Every Time: How to Use the Skills of a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        item_id                                             labels  \\\n",
       "0      0  ID:B0027DQHA0                [Music, Classical, Movies & TV, TV]   \n",
       "1      1  ID:0756400120  [General, Anthologies, United States, Literatu...   \n",
       "2      2  ID:B00024YAOQ  [Business & Investing, Motivation & Self-Impro...   \n",
       "\n",
       "                               title_and_description  \n",
       "0  Sao Paulo Samba (2008) Conducted by John Nesch...  \n",
       "1  Past Imperfect (Daw Book Collectors) This fast...  \n",
       "2  Winning Every Time: How to Use the Skills of a...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at the first 3 rows\n",
    "data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from df to list so it can be processed\n",
    "text = data['title_and_description'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "# VOCAB_SIZE = 2000 \n",
    "VOCAB_SIZE = 200000\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a squence from the tokens\n",
    "sequences = tokenizer.texts_to_sequences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete unused var (to save memory)\n",
    "del text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29260, 21551, 12365, 3328, 4450, 19, 237, 211, 1219, 1, 2781, 7, 2982, 19, 55, 28920, 6087, 3, 1991, 100, 123, 181, 1, 2781, 264, 301, 5176, 1, 3727, 9, 16, 13217, 1255, 3, 6394, 2, 1991, 100, 1500, 131, 499]\n"
     ]
    }
   ],
   "source": [
    "# Have a look at the first sequence\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sequence of integers to sequence of tokens so they can be processed by Word2Vec\n",
    "stringified_sequences = []\n",
    "# for sequence in padded_sequences:\n",
    "for sequence in sequences:\n",
    "    stringified_sequence = [str(index) for index in sequence]\n",
    "    stringified_sequences.append(stringified_sequence)\n",
    "    del stringified_sequence # to save memory??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['29260' '21551' '12365' '3328' '4450' '19' '237' '211' '1219' '1' '2781'\n",
      " '7' '2982' '19' '55' '28920' '6087' '3' '1991' '100' '123' '181' '1'\n",
      " '2781' '264' '301' '5176' '1' '3727' '9' '16' '13217' '1255' '3' '6394'\n",
      " '2' '1991' '100' '1500' '131' '499']\n"
     ]
    }
   ],
   "source": [
    "# Have a look at the first sequence of tokens\n",
    "print(stringified_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the word2vec word vectors\n",
    "In his original CNN-Kim paper, the author used a pre-trained word2vec embedding developed by Google. They provided the link but it's broken. So, we'll create our own word2vec embeddings for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check gensim version used\n",
    "FAST_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\", line 211, in _worker_loop\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, job_parameters, thread_private_mem)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 821, in _do_train_job\n",
      "    tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 628, in gensim.models.word2vec_inner.train_batch_cbow\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\", line 211, in _worker_loop\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, job_parameters, thread_private_mem)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 821, in _do_train_job\n",
      "    tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 628, in gensim.models.word2vec_inner.train_batch_cbow\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "Exception in thread Thread-14:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\", line 211, in _worker_loop\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, job_parameters, thread_private_mem)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 821, in _do_train_job\n",
      "    tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 628, in gensim.models.word2vec_inner.train_batch_cbow\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "\n",
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\", line 211, in _worker_loop\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, job_parameters, thread_private_mem)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 821, in _do_train_job\n",
      "    tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 628, in gensim.models.word2vec_inner.train_batch_cbow\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "Exception in thread Thread-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\", line 211, in _worker_loop\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, job_parameters, thread_private_mem)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 821, in _do_train_job\n",
      "    tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 628, in gensim.models.word2vec_inner.train_batch_cbow\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\", line 211, in _worker_loop\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, job_parameters, thread_private_mem)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 821, in _do_train_job\n",
      "    tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 628, in gensim.models.word2vec_inner.train_batch_cbow\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "Exception in thread Thread-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\", line 211, in _worker_loop\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, job_parameters, thread_private_mem)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 821, in _do_train_job\n",
      "    tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 628, in gensim.models.word2vec_inner.train_batch_cbow\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "Exception in thread Thread-17:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\", line 211, in _worker_loop\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, job_parameters, thread_private_mem)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 821, in _do_train_job\n",
      "    tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 628, in gensim.models.word2vec_inner.train_batch_cbow\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "Exception in thread Thread-16:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\", line 211, in _worker_loop\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, job_parameters, thread_private_mem)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 821, in _do_train_job\n",
      "    tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 628, in gensim.models.word2vec_inner.train_batch_cbow\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "Exception in thread Thread-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\", line 211, in _worker_loop\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, job_parameters, thread_private_mem)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 821, in _do_train_job\n",
      "    tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 628, in gensim.models.word2vec_inner.train_batch_cbow\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "Exception in thread Thread-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\", line 211, in _worker_loop\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, job_parameters, thread_private_mem)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 821, in _do_train_job\n",
      "    tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 628, in gensim.models.word2vec_inner.train_batch_cbow\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "Exception in thread Thread-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\", line 211, in _worker_loop\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, job_parameters, thread_private_mem)\n",
      "  File \"C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\", line 821, in _do_train_job\n",
      "    tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 628, in gensim.models.word2vec_inner.train_batch_cbow\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the word2vec word vectors (200 dimensions)\n",
    "EMBEDDING_DIMENSION = 200\n",
    "word_vectors = Word2Vec(sentences = stringified_sequences,\n",
    "                        sg = 0, # 0 for continuous bag of words model, 1 for skip-gram model\n",
    "                        size = EMBEDDING_DIMENSION, # Dimensionality of the word vectors\n",
    "                        window = 10, # Maximum distance between the current and predicted word within a sentence\n",
    "                        workers = 12, # Use these many worker threads to train the model \n",
    "                        iter = 10) # Run this many time through the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of tokens that have been trained for\n",
    "len(word_vectors.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dimensions of each token\n",
    "word_vectors.wv.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete unused var (to save memory)\n",
    "del stringified_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the word2vec mapping to an embedding matrix\n",
    "An embedding matrix is the structure that TensorFlow will accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty embedding matrix\n",
    "weight_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIMENSION))\n",
    "\n",
    "# Fill the matrix with word vectors\n",
    "for i in range(VOCAB_SIZE - 1):\n",
    "    weight_matrix[i + 1] = word_vectors.wv[str(i + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the embedding matrix shape\n",
    "weight_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the embedding matrix to use for later\n",
    "save_path = '../Datasets/AmazonCat-13K/processed/'\n",
    "np.savetxt(save_path + 'embedding_matrix.csv', weight_matrix, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete unused var (to save memory)\n",
    "del weight_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save a new dataset with the tokenized title and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dataframe\n",
    "tokenized_data = pd.DataFrame(columns = ['item_id', 'tokenized_title_and_description', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the data\n",
    "tokenized_data['item_id'] = data['item_id'].copy()\n",
    "tokenized_data['tokenized_title_and_description'] = sequences # This is the integer version\n",
    "tokenized_data['labels'] = data['labels'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the dataframe\n",
    "tokenized_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the first 3 rows\n",
    "tokenized_data.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column of token counts\n",
    "tokenized_data['token_count'] = tokenized_data['tokenized_title_and_description'].apply(lambda tokens: len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for rows with missing values\n",
    "len(tokenized_data[tokenized_data['token_count'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values\n",
    "tokenized_data = tokenized_data[tokenized_data.token_count != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove column of token counts\n",
    "tokenized_data = tokenized_data.drop('token_count', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the dataframe\n",
    "tokenized_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "tokenized_data = tokenized_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the first 3 rows\n",
    "tokenized_data.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create save_as_csv function\n",
    "def save_as_csv(df, path):\n",
    "    df.to_csv(path, \n",
    "              header=True, \n",
    "              index=None, \n",
    "              encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as csv (broken up into 5 files)\n",
    "num_files = 10\n",
    "size = tokenized_data.shape[0] // num_files\n",
    "for file_num in range(num_files):\n",
    "    if file_num == 0:\n",
    "        save_as_csv(tokenized_data[:size], save_path + f'tokenized_no{file_num + 1}.csv')\n",
    "    elif file_num == (num_files - 1):\n",
    "        save_as_csv(tokenized_data[size * file_num:], save_path + f'tokenized_no{file_num + 1}.csv')\n",
    "    else:\n",
    "        save_as_csv(tokenized_data[size * file_num: size * (file_num + 1)], save_path + f'tokenized_no{file_num + 1}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
