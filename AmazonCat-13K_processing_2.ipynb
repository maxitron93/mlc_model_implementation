{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create embedding matrix\n",
    "We'll use the word2vec module from gensim to create an embedding matrix that can be used by tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from gensim.models.word2vec import FAST_VERSION\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "# data_list = [a, b, c, d, e, f, g, h, i ,f] = [None, None, None, None, None, None, None, None, None, None]\n",
    "data_list = [a] = [None]\n",
    "data_location = '../Datasets/Amazon-Cat13K/processed/' \n",
    "for i in range(len(data_list)):\n",
    "    data_list[i] = pd.read_csv(data_location + f'first_pass_no{i + 1}.csv', encoding='latin1')\n",
    "    \n",
    "# Concatenate all the data and reset the index\n",
    "data = pd.concat(data_list, sort=False)\n",
    "data = data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels from string to array (return unique values only)\n",
    "data['labels'] = data['labels'].apply(lambda labels: list(set(ast.literal_eval(labels))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "data = data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to join title and description\n",
    "def join_title_and_description(row):\n",
    "    return f'{row[\"title\"]} {row[\"description\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column that combines the title and description\n",
    "data['title_and_description'] = data.apply(lambda row: join_title_and_description(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149446, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at the shape\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>item_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>labels</th>\n",
       "      <th>title_and_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ID:B0027DQHA0</td>\n",
       "      <td>Sao Paulo Samba (2008)</td>\n",
       "      <td>Conducted by John Neschling since 1997, the or...</td>\n",
       "      <td>[Music, Movies &amp; TV, Classical, TV]</td>\n",
       "      <td>Sao Paulo Samba (2008) Conducted by John Nesch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ID:0756400120</td>\n",
       "      <td>Past Imperfect (Daw Book Collectors)</td>\n",
       "      <td>This fast, lightweight anthology of 12 time-tr...</td>\n",
       "      <td>[Short Stories, Science Fiction &amp; Fantasy, Ant...</td>\n",
       "      <td>Past Imperfect (Daw Book Collectors) This fast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>ID:B00024YAOQ</td>\n",
       "      <td>Winning Every Time: How to Use the Skills of a...</td>\n",
       "      <td>Whether you're hoping to obtain a raise from y...</td>\n",
       "      <td>[Business Life, Business &amp; Investing, Books, M...</td>\n",
       "      <td>Winning Every Time: How to Use the Skills of a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        item_id                                              title  \\\n",
       "0      0  ID:B0027DQHA0                             Sao Paulo Samba (2008)   \n",
       "1      1  ID:0756400120               Past Imperfect (Daw Book Collectors)   \n",
       "2      2  ID:B00024YAOQ  Winning Every Time: How to Use the Skills of a...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Conducted by John Neschling since 1997, the or...   \n",
       "1  This fast, lightweight anthology of 12 time-tr...   \n",
       "2  Whether you're hoping to obtain a raise from y...   \n",
       "\n",
       "                                              labels  \\\n",
       "0                [Music, Movies & TV, Classical, TV]   \n",
       "1  [Short Stories, Science Fiction & Fantasy, Ant...   \n",
       "2  [Business Life, Business & Investing, Books, M...   \n",
       "\n",
       "                               title_and_description  \n",
       "0  Sao Paulo Samba (2008) Conducted by John Nesch...  \n",
       "1  Past Imperfect (Daw Book Collectors) This fast...  \n",
       "2  Winning Every Time: How to Use the Skills of a...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at the first 3 rows\n",
    "data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from df to list so it can be processed\n",
    "text = data['title_and_description'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "# VOCAB_SIZE = 200000 \n",
    "VOCAB_SIZE = 20000\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a squence from the tokens\n",
    "sequences = tokenizer.texts_to_sequences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13001, 3280, 4417, 19, 239, 214, 1203, 1, 2950, 7, 2916, 19, 55, 6547, 3, 1999, 97, 127, 181, 1, 2950, 265, 304, 5592, 1, 3701, 9, 16, 12325, 1267, 3, 6024, 2, 1999, 97, 1474, 131, 498]\n"
     ]
    }
   ],
   "source": [
    "# Have a look at the first sequence\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sequence of integers to sequence of tokens so they can be processed by Word2Vec\n",
    "stringified_sequences = []\n",
    "# for sequence in padded_sequences:\n",
    "for sequence in sequences:\n",
    "    stringified_sequence = []\n",
    "    for index in sequence:\n",
    "        stringified_sequence.append(str(index))\n",
    "    stringified_sequences.append(stringified_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['13001', '3280', '4417', '19', '239', '214', '1203', '1', '2950', '7', '2916', '19', '55', '6547', '3', '1999', '97', '127', '181', '1', '2950', '265', '304', '5592', '1', '3701', '9', '16', '12325', '1267', '3', '6024', '2', '1999', '97', '1474', '131', '498']\n"
     ]
    }
   ],
   "source": [
    "# Have a look at the first sequence of tokens\n",
    "print(stringified_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the word2vec word vectors\n",
    "In his original CNN-Kim paper, the author used a pre-trained word2vec embedding developed by Google. They provided the link but it's broken. So, we'll create our own word2vec embeddings for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check gensim version used\n",
    "FAST_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the word2vec word vectors (200 dimensions)\n",
    "EMBEDDING_DIMENSION = 200\n",
    "word_vectors = Word2Vec(sentences = stringified_sequences,\n",
    "                        sg = 0, # 0 for continuous bag of words model, 1 for skip-gram model\n",
    "                        size = EMBEDDING_DIMENSION, # Dimensionality of the word vectors\n",
    "                        window = 2, # Maximum distance between the current and predicted word within a sentence\n",
    "                        workers = 12, # Use these many worker threads to train the model \n",
    "                        iter = 2) # Run this many time through the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19999"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of tokens that have been trained for\n",
    "len(word_vectors.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the dimensions of each token\n",
    "word_vectors.wv.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the word2vec mapping to an embedding matrix\n",
    "An embedding matrix is the structure that TensorFlow will accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty embedding matrix\n",
    "weight_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIMENSION))\n",
    "\n",
    "# Fill the matrix with word vectors\n",
    "for i in range(VOCAB_SIZE - 1):\n",
    "    weight_matrix[i + 1] = word_vectors.wv[str(i + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 200)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the embedding matrix shape\n",
    "weight_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the embedding matrix to use for later\n",
    "save_path = '../Datasets/Amazon-Cat13K/processed/'\n",
    "np.savetxt(save_path + 'embedding_matrix.csv', weight_matrix, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save a new dataset with the tokenized title and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dataframe\n",
    "tokenized_data = pd.DataFrame(columns = ['item_id', 'tokenized_title_and_description', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the data\n",
    "tokenized_data['item_id'] = data['item_id'].copy()\n",
    "tokenized_data['tokenized_title_and_description'] = sequences # This is the integer version\n",
    "tokenized_data['labels'] = data['labels'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149446, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the dataframe\n",
    "tokenized_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>tokenized_title_and_description</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ID:B0027DQHA0</td>\n",
       "      <td>[13001, 3280, 4417, 19, 239, 214, 1203, 1, 295...</td>\n",
       "      <td>[Music, Movies &amp; TV, Classical, TV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ID:0756400120</td>\n",
       "      <td>[386, 14728, 40, 5865, 10, 493, 1180, 2959, 3,...</td>\n",
       "      <td>[Short Stories, Science Fiction &amp; Fantasy, Ant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ID:B00024YAOQ</td>\n",
       "      <td>[634, 153, 56, 73, 5, 95, 1, 813, 3, 4, 3385, ...</td>\n",
       "      <td>[Business Life, Business &amp; Investing, Books, M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         item_id                    tokenized_title_and_description  \\\n",
       "0  ID:B0027DQHA0  [13001, 3280, 4417, 19, 239, 214, 1203, 1, 295...   \n",
       "1  ID:0756400120  [386, 14728, 40, 5865, 10, 493, 1180, 2959, 3,...   \n",
       "2  ID:B00024YAOQ  [634, 153, 56, 73, 5, 95, 1, 813, 3, 4, 3385, ...   \n",
       "\n",
       "                                              labels  \n",
       "0                [Music, Movies & TV, Classical, TV]  \n",
       "1  [Short Stories, Science Fiction & Fantasy, Ant...  \n",
       "2  [Business Life, Business & Investing, Books, M...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at the first 3 rows\n",
    "tokenized_data.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column of token counts\n",
    "tokenized_data['token_count'] = tokenized_data['tokenized_title_and_description'].apply(lambda tokens: len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for rows with missing values\n",
    "len(tokenized_data[tokenized_data['token_count'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values\n",
    "tokenized_data = tokenized_data[tokenized_data.token_count != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove column of token counts\n",
    "tokenized_data = tokenized_data.drop('token_count', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149426, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the dataframe\n",
    "tokenized_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "tokenized_data = tokenized_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>tokenized_title_and_description</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ID:B0027DQHA0</td>\n",
       "      <td>[13001, 3280, 4417, 19, 239, 214, 1203, 1, 295...</td>\n",
       "      <td>[Music, Movies &amp; TV, Classical, TV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ID:0756400120</td>\n",
       "      <td>[386, 14728, 40, 5865, 10, 493, 1180, 2959, 3,...</td>\n",
       "      <td>[Short Stories, Science Fiction &amp; Fantasy, Ant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ID:B00024YAOQ</td>\n",
       "      <td>[634, 153, 56, 73, 5, 95, 1, 813, 3, 4, 3385, ...</td>\n",
       "      <td>[Business Life, Business &amp; Investing, Books, M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         item_id                    tokenized_title_and_description  \\\n",
       "0  ID:B0027DQHA0  [13001, 3280, 4417, 19, 239, 214, 1203, 1, 295...   \n",
       "1  ID:0756400120  [386, 14728, 40, 5865, 10, 493, 1180, 2959, 3,...   \n",
       "2  ID:B00024YAOQ  [634, 153, 56, 73, 5, 95, 1, 813, 3, 4, 3385, ...   \n",
       "\n",
       "                                              labels  \n",
       "0                [Music, Movies & TV, Classical, TV]  \n",
       "1  [Short Stories, Science Fiction & Fantasy, Ant...  \n",
       "2  [Business Life, Business & Investing, Books, M...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at the first 3 rows\n",
    "tokenized_data.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create save_as_csv function\n",
    "def save_as_csv(df, path):\n",
    "    df.to_csv(path, \n",
    "              header=True, \n",
    "              index=None, \n",
    "              encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as csv (broken up into 5 files)\n",
    "num_files = 10\n",
    "size = tokenized_data.shape[0] // num_files\n",
    "for file_num in range(num_files):\n",
    "    if file_num == 0:\n",
    "        save_as_csv(tokenized_data[:size], save_path + f'tokenized_no{file_num + 1}.csv')\n",
    "    elif file_num == (num_files - 1):\n",
    "        save_as_csv(tokenized_data[size * file_num:], save_path + f'tokenized_no{file_num + 1}.csv')\n",
    "    else:\n",
    "        save_as_csv(tokenized_data[size * file_num: size * (file_num + 1)], save_path + f'tokenized_no{file_num + 1}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
