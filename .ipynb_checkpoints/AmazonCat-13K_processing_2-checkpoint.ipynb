{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the word vectors\n",
    "We'll use the word2vec module from gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import pandas as pd\n",
    "import ast\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "# data_list = [a, b, c, d, e, f, g, h, i ,f] = [None, None, None, None, None, None, None, None, None, None]\n",
    "data_list = [a] = [None]\n",
    "data_location = '../Datasets/Amazon-Cat13K/processed/' \n",
    "for i in range(len(data_list)):\n",
    "    data_list[i] = pd.read_csv(data_location + f'first_pass_no{i + 1}.csv', encoding='latin1')\n",
    "    \n",
    "# Concatenate all the data\n",
    "data = pd.concat(data_list, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels from string to array (return unique values only)\n",
    "data['labels'] = data['labels'].apply(lambda labels: list(set(ast.literal_eval(labels))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "data = data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to join title and description\n",
    "def join_title_and_description(row):\n",
    "    return row['title'] + ' ' + row['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column that combines the title and description\n",
    "data['title_and_description'] = data.apply(lambda row: join_title_and_description(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149446, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at the shape\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>labels</th>\n",
       "      <th>title_and_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ID:B0027DQHA0</td>\n",
       "      <td>Sao Paulo Samba (2008)</td>\n",
       "      <td>Conducted by John Neschling since 1997, the or...</td>\n",
       "      <td>[Movies &amp; TV, TV, Music, Classical]</td>\n",
       "      <td>Sao Paulo Samba (2008) Conducted by John Nesch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ID:0756400120</td>\n",
       "      <td>Past Imperfect (Daw Book Collectors)</td>\n",
       "      <td>This fast, lightweight anthology of 12 time-tr...</td>\n",
       "      <td>[Science Fiction, Short Stories, Literature &amp; ...</td>\n",
       "      <td>Past Imperfect (Daw Book Collectors) This fast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ID:B00024YAOQ</td>\n",
       "      <td>Winning Every Time: How to Use the Skills of a...</td>\n",
       "      <td>Whether you're hoping to obtain a raise from y...</td>\n",
       "      <td>[Books, Business &amp; Investing, Business Life, M...</td>\n",
       "      <td>Winning Every Time: How to Use the Skills of a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ID:B000BUGXAU</td>\n",
       "      <td>Nano Cube 24 Gallon Deluxe</td>\n",
       "      <td>Just add water!\\tThe Nano Cube is a 24-gallon ...</td>\n",
       "      <td>[Pet Supplies, Fish &amp; Aquatic Pets, Aquariums]</td>\n",
       "      <td>Nano Cube 24 Gallon Deluxe Just add water!\\tTh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ID:B0007YMWC8</td>\n",
       "      <td>Asalto En Tijuana (2005)</td>\n",
       "      <td>An honest citizen is forced to steal the world...</td>\n",
       "      <td>[Movies &amp; TV, Movies]</td>\n",
       "      <td>Asalto En Tijuana (2005) An honest citizen is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         item_id                                              title  \\\n",
       "0  ID:B0027DQHA0                             Sao Paulo Samba (2008)   \n",
       "1  ID:0756400120               Past Imperfect (Daw Book Collectors)   \n",
       "2  ID:B00024YAOQ  Winning Every Time: How to Use the Skills of a...   \n",
       "3  ID:B000BUGXAU                         Nano Cube 24 Gallon Deluxe   \n",
       "4  ID:B0007YMWC8                           Asalto En Tijuana (2005)   \n",
       "\n",
       "                                         description  \\\n",
       "0  Conducted by John Neschling since 1997, the or...   \n",
       "1  This fast, lightweight anthology of 12 time-tr...   \n",
       "2  Whether you're hoping to obtain a raise from y...   \n",
       "3  Just add water!\\tThe Nano Cube is a 24-gallon ...   \n",
       "4  An honest citizen is forced to steal the world...   \n",
       "\n",
       "                                              labels  \\\n",
       "0                [Movies & TV, TV, Music, Classical]   \n",
       "1  [Science Fiction, Short Stories, Literature & ...   \n",
       "2  [Books, Business & Investing, Business Life, M...   \n",
       "3     [Pet Supplies, Fish & Aquatic Pets, Aquariums]   \n",
       "4                              [Movies & TV, Movies]   \n",
       "\n",
       "                               title_and_description  \n",
       "0  Sao Paulo Samba (2008) Conducted by John Nesch...  \n",
       "1  Past Imperfect (Daw Book Collectors) This fast...  \n",
       "2  Winning Every Time: How to Use the Skills of a...  \n",
       "3  Nano Cube 24 Gallon Deluxe Just add water!\\tTh...  \n",
       "4  Asalto En Tijuana (2005) An honest citizen is ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at the data\n",
    "data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from df to list\n",
    "text = data['title_and_description'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "# VOCAB_SIZE = 203882 # This is the reported number of features \n",
    "VOCAB_SIZE = 10000\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a squence from the tokens\n",
    "sequences = tokenizer.texts_to_sequences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3280, 4417, 19, 239, 214, 1203, 1, 2950, 7, 2916, 19, 55, 6547, 3, 1999, 97, 127, 181, 1, 2950, 265, 304, 5592, 1, 3701, 9, 16, 1267, 3, 6024, 2, 1999, 97, 1474, 131, 498]\n"
     ]
    }
   ],
   "source": [
    "# Have a look at the first sequence\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that 0 is unused\n",
    "0 in tokenizer.index_word.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding to the sequences\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "padded_sequences = pad_sequences(sequences,\n",
    "                                 maxlen=MAX_SEQUENCE_LENGTH,\n",
    "                                 padding='post') # Add padding to the end if needs padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3280 4417   19  239  214 1203    1 2950    7 2916   19   55 6547    3\n",
      " 1999   97  127  181    1 2950  265  304 5592    1 3701    9   16 1267\n",
      "    3 6024    2 1999   97 1474  131  498    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# Have a look at the first sequence\n",
    "print(padded_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding to the list of tokens \n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sequence of integers to sequence of tokens\n",
    "token_sequences = []\n",
    "for sequence in padded_sequences:\n",
    "    token_sequence = []\n",
    "    for index in sequence:\n",
    "        token_sequence.append(tokenizer.index_word[index])\n",
    "    token_sequences.append(token_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2008', 'conducted', 'by', 'john', 'since', '1997', 'the', 'orchestra', 'is', 'defined', 'by', 'its', 'interpretations', 'of', 'latin', 'american', 'music', 'here', 'the', 'orchestra', 'yet', 'again', 'grips', 'the', 'listener', 'with', 'an', 'selection', 'of', 'brazilian', 'and', 'latin', 'american', 'classics', 'including', 'w', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# Have a look at the first sequence of tokens\n",
    "print(token_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the word2vec word vectors\n",
    "In his original CNN-Kim paper, the author used a pre-trained word2vec embedding developed by Google. They provided the link but it's broken. So, we'll create our own word2vec embeddings for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the word2vec word embeddings (50 dimensions)\n",
    "word2vec_word_embeddings = Word2Vec(sentences = token_sequences,\n",
    "                           sg = 0, # 0 for continuous bag of words model, 1 for skip-gram model\n",
    "                           size = 300, # Dimensionality of the word vectors\n",
    "                           window = 5, # Maximum distance between the current and predicted word within a sentence\n",
    "                           workers = 10, # Use these many worker threads to train the model\n",
    "                           iter = 5) # Number of iterations (epochs) over the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of tokens that have been trained for\n",
    "len(word2vec_word_embeddings.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the dimensions of each token\n",
    "word2vec_word_embeddings.wv.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gameplay', 0.5068792104721069),\n",
       " ('batman', 0.48384609818458557),\n",
       " ('adventure', 0.46568042039871216),\n",
       " ('suspense', 0.46249258518218994),\n",
       " ('sequences', 0.4161718487739563),\n",
       " ('combat', 0.4145388603210449),\n",
       " ('animation', 0.4104458689689636),\n",
       " ('superman', 0.40703117847442627),\n",
       " ('mayhem', 0.40631985664367676),\n",
       " ('climax', 0.4053933024406433)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at the words that are most similar to 'action'\n",
    "word2vec_word_embeddings.wv.most_similar('action')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.19863790e-01 -1.51247203e+00 -2.39675546e+00  1.26724958e+00\n",
      "  1.42668235e+00  1.17790198e+00 -3.58591318e-01  1.96082257e-02\n",
      "  1.72476992e-01 -1.01584601e+00  5.88264287e-01  3.07916820e-01\n",
      " -4.09925699e-01  1.14585117e-01  6.50731381e-05  1.85358375e-01\n",
      "  8.49683940e-01  1.04901217e-01 -3.70536074e-02  1.98649690e-01\n",
      " -2.24202537e-04 -3.78435940e-01 -5.53013645e-02  5.38876534e-01\n",
      "  5.75046122e-01 -9.45357025e-01  7.90321231e-01 -1.74390852e-01\n",
      " -5.28430939e-01  9.73555326e-01  5.63493650e-03  8.87058258e-01\n",
      " -1.13490546e+00  2.61756450e-01 -5.24946094e-01  8.54004622e-01\n",
      "  2.84199357e-01  7.81403780e-01  1.03920424e+00  8.72253656e-01\n",
      "  1.63375735e+00 -9.58525062e-01  2.76313007e-01  3.03469419e-01\n",
      "  4.07338619e-01 -1.47377133e-01  1.54876113e-01  2.22706628e+00\n",
      "  4.91876006e-01  2.67607421e-01  6.86423779e-01 -1.32721737e-01\n",
      "  2.29413897e-01  1.63715646e-01 -8.65637720e-01 -1.16545832e+00\n",
      " -7.34755635e-01  9.45155442e-01  1.64421409e-01  8.67005885e-01\n",
      "  3.94191265e-01  1.28433537e+00 -1.73949033e-01 -4.01213050e-01\n",
      " -4.51787174e-01 -2.71823287e-01 -7.49432147e-02 -2.16591311e+00\n",
      " -9.26212013e-01 -1.05353749e+00 -9.68702286e-02  1.13373220e-01\n",
      "  3.54442298e-01 -8.20297599e-01 -6.83510661e-01  1.36770070e-01\n",
      "  9.10677552e-01 -7.58257031e-01 -3.18147808e-01 -1.91900432e-01\n",
      "  2.05945820e-01 -8.88048559e-02 -2.72686899e-01  1.66892231e+00\n",
      "  1.43841660e+00  9.88845944e-01  3.31017166e-01  9.23463181e-02\n",
      " -8.84949505e-01 -1.80163845e-01  1.18416154e+00  1.70027256e+00\n",
      " -1.23129256e-01 -8.50013077e-01 -5.70309103e-01 -1.12234116e+00\n",
      "  1.55209696e+00  4.47600245e-01  9.45097566e-01  7.98189044e-01\n",
      " -1.14508188e+00  5.78605354e-01 -2.74486393e-02 -5.25083482e-01\n",
      "  1.33909798e+00  1.02437846e-03  6.49931133e-01  3.04473132e-01\n",
      "  1.51173830e+00  3.11257750e-01 -3.27352226e-01 -4.94144320e-01\n",
      " -1.99952102e+00 -6.90545738e-01 -4.48443621e-01 -4.36946154e-01\n",
      "  3.61053050e-01  2.24985749e-01 -9.16740596e-01 -1.97570905e-01\n",
      " -1.34206700e+00 -3.08765233e-01 -2.77073056e-01  4.18814301e-01\n",
      "  3.78406435e-01  1.25407326e+00  4.08902586e-01 -1.35063708e+00\n",
      " -1.53530526e+00  5.14516830e-01 -4.65151528e-03  1.85600615e+00\n",
      " -3.42685610e-01  3.66906226e-01 -2.49672160e-01  1.06826138e+00\n",
      "  1.27927605e-02 -1.17880613e-01  3.15452218e-01 -9.04026687e-01\n",
      " -2.83813357e-01  5.14152348e-01 -6.01608045e-02 -2.96776414e-01\n",
      "  1.51320592e-01  8.30903947e-01 -1.23292959e+00 -6.29426837e-01\n",
      "  1.04577947e+00 -6.90330505e-01 -1.24126643e-01  1.10536385e+00\n",
      " -7.00827897e-01 -3.84350628e-01  2.13893437e+00 -5.00899367e-03\n",
      "  3.79093379e-01  5.95379591e-01 -5.80692232e-01  1.31010640e+00\n",
      " -1.10692763e+00  1.43050432e-01  2.15099975e-01 -8.79444256e-02\n",
      " -1.41885906e-01 -6.46150768e-01 -1.62432298e-01 -7.01045156e-01\n",
      "  1.02062297e+00  5.05539358e-01 -7.12736428e-01  9.67532218e-01\n",
      "  5.17096341e-01  4.58467513e-01  1.86202121e+00  1.43788354e-02\n",
      "  7.17268944e-01 -7.76729822e-01 -5.00603139e-01 -5.87851465e-01\n",
      " -1.04876876e+00 -7.49009669e-01  6.66612014e-02  5.14212072e-01\n",
      "  6.36855543e-01  5.41377068e-01  1.14442265e+00 -6.67732731e-02\n",
      " -3.65375519e-01 -5.27580082e-01 -8.10067296e-01  1.94377378e-01\n",
      " -1.25808075e-01  3.67771327e-01 -2.99351484e-01  3.28185745e-02\n",
      "  9.70313311e-01 -1.08810794e+00 -1.46947968e+00 -4.63568687e-01\n",
      " -1.76827645e+00 -1.39480126e+00  1.35588574e+00  1.34024715e+00\n",
      " -1.00039101e+00 -1.37549609e-01 -3.40383828e-01  1.19350898e+00\n",
      "  1.60750091e+00  1.03670692e+00 -3.61330122e-01  8.90672922e-01\n",
      "  9.98432755e-01 -9.39656913e-01 -3.06505769e-01  1.56374669e+00\n",
      "  1.76304793e+00 -2.22273946e-01  7.48127997e-01 -7.08046854e-02\n",
      "  1.23635793e+00  7.33225524e-01 -3.64855468e-01  1.32459877e-02\n",
      "  1.05513728e+00 -1.30019045e+00 -1.11485672e+00  6.61343038e-01\n",
      " -7.74209976e-01  9.25547183e-01  9.63613033e-01 -8.99730682e-01\n",
      " -2.58886546e-01  2.60819912e-01 -1.59062850e+00  5.54564834e-01\n",
      "  1.13504231e+00 -1.89722016e-01  1.65221214e+00 -4.27176714e-01\n",
      " -2.24002552e+00 -1.35125840e+00  1.00456429e+00  5.10652065e-01\n",
      "  1.58476508e+00  4.86518621e-01 -7.55660713e-01  2.77944896e-02\n",
      "  4.87072527e-01  1.31096697e+00  1.07888842e+00 -6.90824747e-01\n",
      "  1.14176586e-01 -1.52898264e+00 -9.47849929e-01  2.77967930e-01\n",
      "  1.51144373e+00  6.78599834e-01 -4.36477810e-01 -1.10857522e+00\n",
      " -1.50957286e-01  5.31928241e-01 -1.26224768e+00  5.95874228e-02\n",
      "  1.35255349e+00 -5.51697016e-01 -6.26731887e-02  8.22603628e-02\n",
      "  2.19178581e+00 -3.49116653e-01 -1.47485554e-01  9.80925024e-01\n",
      "  7.39438832e-01  1.02960598e+00  1.87854752e-01 -1.12291956e+00\n",
      "  9.22127724e-01  1.26772836e-01 -1.11870360e+00  2.37164670e-03\n",
      " -5.72146714e-01  9.58538353e-01 -1.86526284e-01 -1.21935964e+00\n",
      " -9.14444685e-01 -7.07508743e-01 -4.68605086e-02 -8.00688386e-01\n",
      " -8.56646478e-01 -2.35799581e-01 -1.50307155e+00 -1.15486431e+00\n",
      "  1.85847700e-01  1.10516775e+00 -2.99042940e-01 -4.76566792e-01\n",
      "  8.82930994e-01 -7.45946586e-01 -5.18598199e-01  8.15337241e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxitron\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Have a look at the word embedding for 'action'\n",
    "print(word2vec_word_embeddings['action']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word vectors\n",
    "word2vec_word_embeddings.wv.save('../Datasets/Amazon-Cat13K/processed/word_vectors.kv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save a new dataset with the tokenized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dataframe\n",
    "tokenized_data = pd.DataFrame(columns = ['item_id', 'tokenized_title_and_description', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the data\n",
    "tokenized_data['item_id'] = data['item_id'].copy()\n",
    "tokenized_data['tokenized_title_and_description'] = token_sequences\n",
    "tokenized_data['labels'] = data['labels'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>tokenized_title_and_description</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ID:B0027DQHA0</td>\n",
       "      <td>[2008, conducted, by, john, since, 1997, the, ...</td>\n",
       "      <td>[Movies &amp; TV, TV, Music, Classical]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ID:0756400120</td>\n",
       "      <td>[machine, but, a, future, version, of, himself...</td>\n",
       "      <td>[Science Fiction, Short Stories, Literature &amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ID:B00024YAOQ</td>\n",
       "      <td>[winning, every, time, how, to, use, the, skil...</td>\n",
       "      <td>[Books, Business &amp; Investing, Business Life, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ID:B000BUGXAU</td>\n",
       "      <td>[nano, cube, 24, gallon, deluxe, just, add, wa...</td>\n",
       "      <td>[Pet Supplies, Fish &amp; Aquatic Pets, Aquariums]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ID:B0007YMWC8</td>\n",
       "      <td>[en, 2005, an, honest, citizen, is, forced, to...</td>\n",
       "      <td>[Movies &amp; TV, Movies]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         item_id                    tokenized_title_and_description  \\\n",
       "0  ID:B0027DQHA0  [2008, conducted, by, john, since, 1997, the, ...   \n",
       "1  ID:0756400120  [machine, but, a, future, version, of, himself...   \n",
       "2  ID:B00024YAOQ  [winning, every, time, how, to, use, the, skil...   \n",
       "3  ID:B000BUGXAU  [nano, cube, 24, gallon, deluxe, just, add, wa...   \n",
       "4  ID:B0007YMWC8  [en, 2005, an, honest, citizen, is, forced, to...   \n",
       "\n",
       "                                              labels  \n",
       "0                [Movies & TV, TV, Music, Classical]  \n",
       "1  [Science Fiction, Short Stories, Literature & ...  \n",
       "2  [Books, Business & Investing, Business Life, M...  \n",
       "3     [Pet Supplies, Fish & Aquatic Pets, Aquariums]  \n",
       "4                              [Movies & TV, Movies]  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at the first 5 rows\n",
    "tokenized_data.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create save_as_csv function\n",
    "def save_as_csv(df, path):\n",
    "    df.to_csv(path, \n",
    "              header=True, \n",
    "              index=None, \n",
    "              encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as csv (broken up into 5 files)\n",
    "num_files = 10\n",
    "size = tokenized_data.shape[0] // num_files\n",
    "save_path = '../Datasets/Amazon-Cat13K/processed/tokenized'\n",
    "for file_num in range(num_files):\n",
    "    if file_num == 0:\n",
    "        save_as_csv(tokenized_data[:size], save_path + f'_no{file_num + 1}.csv')\n",
    "    elif file_num == (num_files - 1):\n",
    "        save_as_csv(tokenized_data[size * file_num:], save_path + f'_no{file_num + 1}.csv')\n",
    "    else:\n",
    "        save_as_csv(tokenized_data[size * file_num: size * (file_num + 1)], save_path + f'_no{file_num + 1}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "# test = KeyedVectors.load('../Datasets/Amazon-Cat13K/processed/word_vectors.kv', mmap='r')\n",
    "# test['action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
